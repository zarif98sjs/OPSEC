{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Minh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Minh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Minh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from collections import defaultdict\n",
    "import gensim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.mixture import GaussianMixture\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowsPath('C:/Users/Minh/OneDrive - Auburn University/A2IMOOC/Results/Production')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get parent working directory\n",
    "from pathlib import Path\n",
    "parent = Path.cwd().parents[1]\n",
    "parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def pre_processData(newsgroups_train):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    for i in range(len(newsgroups_train)):\n",
    "        newsgroups_train[i] = (newsgroups_train[i]).lower()\n",
    "        newsgroups_train[i] = tokenizer.tokenize(newsgroups_train[i])\n",
    "    newsgroups_train = [[token for token in doc if not token.isdigit()] for doc in newsgroups_train]\n",
    "    #newsgroups_train = [[token for token in doc if len(token) > 3] for doc in newsgroups_train]\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    newsgroups_train = [[lemmatizer.lemmatize(token) for token in doc] for doc in newsgroups_train]\n",
    "    return newsgroups_train\n",
    "\n",
    "\n",
    "def remove_stopwords(documents,news_labels):\n",
    "    temp_corpus = {}\n",
    "    temp_label = {}\n",
    "    stop_en = stopwords.words('english')\n",
    "    i = 0\n",
    "    for index, words in enumerate(documents):\n",
    "        rwords=[]\n",
    "        for word in words:\n",
    "            if word not in stop_en:\n",
    "                rwords.append(word)\n",
    "        if rwords:\n",
    "            temp_corpus[i] = rwords\n",
    "            temp_label[i] = news_labels[index]\n",
    "            i = i+1\n",
    "        else:\n",
    "            temp_corpus[i] = words\n",
    "            temp_label[i] = news_labels[index]\n",
    "            i = i+1\n",
    "    return temp_corpus,temp_label\n",
    "\n",
    "def process_wordvectors(vocab,vectors,documents,news_labels):\n",
    "    useable_vocab = 0\n",
    "    unusable_vocab = 0\n",
    "    temp_corpus = {}\n",
    "    temp_labels = {}\n",
    "    temp_unvecs = {}\n",
    "    i=0\n",
    "    for index in range(len(documents)):\n",
    "        filter_word = []\n",
    "        words = documents[index]\n",
    "        temp_unvecs[i] = {}\n",
    "        for word in words:\n",
    "            try:\n",
    "                vectors[word]\n",
    "                vocab.add(word)\n",
    "                filter_word.append(word)\n",
    "                useable_vocab += 1\n",
    "            except:\n",
    "                unusable_vocab += 1\n",
    "                if word in temp_unvecs[i].keys():\n",
    "                    temp_unvecs[i][word] +=1\n",
    "                else:\n",
    "                     temp_unvecs[i][word] = 1\n",
    "                continue\n",
    "        if filter_word and len(words)>0 :\n",
    "            temp_corpus[i] = filter_word\n",
    "            temp_labels[i] = news_labels[index]\n",
    "            #print(\"doc \",i,\" not vec word len \",temp_unvecs[i])\n",
    "            i= i+1\n",
    "        else:\n",
    "            temp_corpus[i] = 'unk'\n",
    "            temp_labels[i] = news_labels[index]\n",
    "            i= i+1\n",
    "    print(\"There are {0} words that could be convereted to word vectors in your corpus \\n\" \\\n",
    "          \"There are {1} words that could NOT be converted to word vectors\".format(useable_vocab, unusable_vocab))\n",
    "    print(\"doc num: \",i)\n",
    "    print(\"label num: \",i)\n",
    "    return temp_corpus,temp_labels,temp_unvecs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "wordvec_filepath = \"glove.6B.50d.txt\"\n",
    "#remove = ('headers', 'footers', 'quotes')\n",
    "#fectch_corpus = fetch_20newsgroups(subset='train', remove=remove)\n",
    "#newsgroups_train = fectch_corpus.data\n",
    "#print(\"target_names len \",len(fectch_corpus.target_names))\n",
    "#print(\"newsgroups_train size \",len(newsgroups_train))\n",
    "#print(\"newsgroups_train type \",type(newsgroups_train) )\n",
    "docids = list()\n",
    "newsgroups_train = list()\n",
    "news_labels = list()\n",
    "supwords_train = list()\n",
    "file_path= parent / 'Data/Tweets-T'\n",
    "#file_path=\"C:/Users/aa6602623/PycharmProjects/GaussianLDA/News.txt\"\n",
    "with open(file_path) as fp:\n",
    "    lines = fp.read().split(\"\\n\")\n",
    "    for line in lines:\n",
    "        if line:\n",
    "            docid = json.loads(line)[\"Id\"]\n",
    "            text = json.loads(line)[\"textCleaned\"].strip()\n",
    "            label = json.loads(line)[\"clusterNo\"]\n",
    "            docids.append(docid)\n",
    "            newsgroups_train.append(text)\n",
    "            news_labels.append(label)\n",
    "fp.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = pre_processData(newsgroups_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus,news_labels = remove_stopwords(corpus,news_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'glove.6B.50d.txt'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordvec_filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = gensim.models.KeyedVectors.load_word2vec_format(fname=wordvec_filepath, binary=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 233710 words that could be convereted to word vectors in your corpus \n",
      "There are 5372 words that could NOT be converted to word vectors\n",
      "doc num:  30322\n",
      "label num:  30322\n",
      "30322\n",
      "30322\n"
     ]
    }
   ],
   "source": [
    "vocab = set([])\n",
    "corpus,news_labels,supwords_train =  process_wordvectors(vocab,vectors,corpus,news_labels)\n",
    "print(len(news_labels))\n",
    "print(len(corpus))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30322"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10448"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit\n",
    "from scipy.special import gamma, gammaln, loggamma\n",
    "from numpy import log, pi, linalg, exp, e\n",
    "import random\n",
    "from scipy import stats\n",
    "stats.chisqprob = lambda chisq, df: stats.chi2.sf(chisq, df)\n",
    "import sys\n",
    "doc_words = {}\n",
    "for docID in range(len(corpus)):\n",
    "    doc_words[docID] = {}\n",
    "    words = corpus[docID]\n",
    "    for word in words:\n",
    "        if word not in doc_words[docID].keys():\n",
    "            doc_words[docID][word] = 0\n",
    "        doc_words[docID][word] += 1\n",
    "        \n",
    "alpha = 0.03\n",
    "beta = 0.03\n",
    "docID_assign_z = {}\n",
    "m_z = {}\n",
    "n_z = {}\n",
    "n_w = {}\n",
    "Topics = []\n",
    "docID = 0\n",
    "initial_z = 0\n",
    "V = set()\n",
    "D = set()\n",
    "beta_topic_sum = {}\n",
    "beta_topic_v = {}\n",
    "gamma = 0.03\n",
    "gammaS = 0.0000001\n",
    "\n",
    "D.add(docID)\n",
    "docID_assign_z[docID] = initial_z\n",
    "words = corpus[docID]\n",
    "if initial_z not in m_z.keys():\n",
    "    m_z[initial_z] = set()\n",
    "m_z[initial_z].add(docID)\n",
    "for word in words:\n",
    "    if initial_z not in n_w.keys():\n",
    "        n_w[initial_z] = 0\n",
    "    if initial_z not in n_z.keys():\n",
    "        n_z[initial_z] = {}\n",
    "    if word not in n_z[initial_z].keys():\n",
    "        n_z[initial_z][word] = 0\n",
    "    n_z[initial_z][word] += 1\n",
    "    n_w[initial_z] += 1\n",
    "    V.add(word)\n",
    "def sum_topic_word():\n",
    "    global_important_word = []\n",
    "    for k in topic_keyword.keys():\n",
    "        global_important_word.extend(list(topic_keyword[k]))\n",
    "    x = None\n",
    "    if len(global_important_word) !=0:\n",
    "        vec_dim = 0\n",
    "        for word in global_important_word:\n",
    "            vec_dim += 1\n",
    "            if x is not None:\n",
    "                x = np.row_stack((x, vectors[word]))\n",
    "            else:\n",
    "                x = vectors[word] \n",
    "        global_v_bar_k[0] = None\n",
    "        if  vec_dim > 1:\n",
    "            global_v_bar_k[0]= np.mean(x,axis = 0)[:,None]\n",
    "        else:\n",
    "            x = x[:,None]\n",
    "            global_v_bar_k[0] = x\n",
    "        x = x.T\n",
    "        global_kappa_k[0] =  kappa0 + vec_dim\n",
    "        global_N[0] = vec_dim\n",
    "        global_C_k[0] = (x - global_v_bar_k[0]).dot((x - global_v_bar_k[0]).T)\n",
    "        global_mu_k[0] = (kappa0 * mu0 + global_N[0] * global_v_bar_k[0]) / global_kappa_k[0]\n",
    "        global_psi_k[0] = psi + global_C_k[0] + (kappa0*global_N[0] / global_kappa_k[0])* ( (global_v_bar_k[0] - mu0).T.dot(global_v_bar_k[0] - mu0) )\n",
    "        global_nu_k[0] = nu0 + vec_dim\n",
    "        shakage_v = vec_dim + 1\n",
    "        global_cov_k[0] = global_psi_k[0] / (shakage_v) \n",
    "        global_inv_cov_k[0] = np.linalg.inv(global_cov_k[0])\n",
    "        #global_cov_det_k[0] = np.linalg.det(global_cov_k[0])\n",
    "    else:\n",
    "        shakage_v = 20\n",
    "        global_cov_k[0] = psi/ (shakage_v) \n",
    "        global_mu_k[0] = mu0\n",
    "        global_inv_cov_k[0] = np.linalg.inv(global_cov_k[0])\n",
    "        #global_cov_det_k[0] = np.linalg.det(global_cov_k[0])\n",
    "    return global_important_word\n",
    "def sampleBetaAssignment(k, word,iter,total_iter, max_word_prob):\n",
    "    if beta_topic_v[k][word] == 1:\n",
    "        beta_topic_sum[k] -= 1\n",
    "    pBetaAllOthers = beta_topic_sum[k]\n",
    "    \n",
    "    log_true = (n_z[k][word] / n_w[k]) / max_word_prob\n",
    "    log_false = 1 - log_true\n",
    "    \n",
    "    log_p = []\n",
    "    if log_false <0 or log_true < 0:\n",
    "        a = 1/0\n",
    "    log_p.append(log_false)\n",
    "    log_p.append(log_true)\n",
    "    \n",
    "    sum_pro=sum(log_p)\n",
    "    normalized_posterior = [i/sum_pro for i in log_p]    \n",
    "    update_k = np.random.choice( 2 , 1, p=normalized_posterior)[0]\n",
    "    if iter ==  total_iter - 1:\n",
    "        update_k = 0\n",
    "        if log_false < log_true:\n",
    "            update_k = 1\n",
    "    if update_k == 1:\n",
    "        beta_topic_v[k][word] = 1\n",
    "        beta_topic_sum[k] += 1\n",
    "        topic_keyword[k].add(word)\n",
    "    else:\n",
    "        beta_topic_v[k][word] = 0\n",
    "        topic_keyword[k].discard(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "compara_batch = [len(corpus)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter  0  total K  0\n",
      "iter  0  doc  0\n",
      "total K  0\n",
      "iter  0  doc  100\n",
      "total K  89\n",
      "iter  0  doc  200\n",
      "total K  92\n",
      "iter  0  doc  300\n",
      "total K  94\n",
      "iter  0  doc  400\n",
      "total K  96\n",
      "iter  0  doc  500\n",
      "total K  99\n",
      "iter  0  doc  600\n",
      "total K  102\n",
      "iter  0  doc  700\n",
      "total K  104\n",
      "iter  0  doc  800\n",
      "total K  106\n",
      "iter  0  doc  900\n",
      "total K  107\n",
      "iter  0  doc  1000\n",
      "total K  107\n",
      "iter  0  doc  1100\n",
      "total K  108\n",
      "iter  0  doc  1200\n",
      "total K  110\n",
      "iter  0  doc  1300\n",
      "total K  112\n",
      "iter  0  doc  1400\n",
      "total K  112\n",
      "iter  0  doc  1500\n",
      "total K  112\n",
      "iter  0  doc  1600\n",
      "total K  114\n",
      "iter  0  doc  1700\n",
      "total K  115\n",
      "iter  0  doc  1800\n",
      "total K  117\n",
      "iter  0  doc  1900\n",
      "total K  118\n",
      "iter  0  doc  2000\n",
      "total K  132\n",
      "iter  0  doc  2100\n",
      "total K  136\n",
      "iter  0  doc  2200\n",
      "total K  136\n",
      "iter  0  doc  2300\n",
      "total K  141\n",
      "iter  0  doc  2400\n",
      "total K  142\n",
      "iter  0  doc  2500\n",
      "total K  143\n",
      "iter  0  doc  2600\n",
      "total K  143\n",
      "iter  0  doc  2700\n",
      "total K  144\n",
      "iter  0  doc  2800\n",
      "total K  147\n",
      "iter  0  doc  2900\n",
      "total K  149\n",
      "iter  0  doc  3000\n",
      "total K  152\n",
      "iter  0  doc  3100\n",
      "total K  153\n",
      "iter  0  doc  3200\n",
      "total K  156\n",
      "iter  0  doc  3300\n",
      "total K  156\n",
      "iter  0  doc  3400\n",
      "total K  156\n",
      "iter  0  doc  3500\n",
      "total K  156\n",
      "iter  0  doc  3600\n",
      "total K  159\n",
      "iter  0  doc  3700\n",
      "total K  159\n",
      "iter  0  doc  3800\n",
      "total K  162\n",
      "iter  0  doc  3900\n",
      "total K  175\n",
      "iter  0  doc  4000\n",
      "total K  179\n",
      "iter  0  doc  4100\n",
      "total K  180\n",
      "iter  0  doc  4200\n",
      "total K  184\n",
      "iter  0  doc  4300\n",
      "total K  184\n",
      "iter  0  doc  4400\n",
      "total K  184\n",
      "iter  0  doc  4500\n",
      "total K  185\n",
      "iter  0  doc  4600\n",
      "total K  186\n",
      "iter  0  doc  4700\n",
      "total K  188\n",
      "iter  0  doc  4800\n",
      "total K  188\n",
      "iter  0  doc  4900\n",
      "total K  188\n",
      "iter  0  doc  5000\n",
      "total K  188\n",
      "iter  0  doc  5100\n",
      "total K  189\n",
      "iter  0  doc  5200\n",
      "total K  189\n",
      "iter  0  doc  5300\n",
      "total K  190\n",
      "iter  0  doc  5400\n",
      "total K  193\n",
      "iter  0  doc  5500\n",
      "total K  193\n",
      "iter  0  doc  5600\n",
      "total K  193\n",
      "iter  0  doc  5700\n",
      "total K  195\n",
      "iter  0  doc  5800\n",
      "total K  208\n",
      "iter  0  doc  5900\n",
      "total K  217\n",
      "iter  0  doc  6000\n",
      "total K  220\n",
      "iter  0  doc  6100\n",
      "total K  223\n",
      "iter  0  doc  6200\n",
      "total K  225\n",
      "iter  0  doc  6300\n",
      "total K  225\n",
      "iter  0  doc  6400\n",
      "total K  227\n",
      "iter  0  doc  6500\n",
      "total K  228\n",
      "iter  0  doc  6600\n",
      "total K  228\n",
      "iter  0  doc  6700\n",
      "total K  229\n",
      "iter  0  doc  6800\n",
      "total K  229\n",
      "iter  0  doc  6900\n",
      "total K  230\n",
      "iter  0  doc  7000\n",
      "total K  230\n",
      "iter  0  doc  7100\n",
      "total K  234\n",
      "iter  0  doc  7200\n",
      "total K  234\n",
      "iter  0  doc  7300\n",
      "total K  234\n",
      "iter  0  doc  7400\n",
      "total K  234\n",
      "iter  0  doc  7500\n",
      "total K  235\n",
      "iter  0  doc  7600\n",
      "total K  236\n",
      "iter  0  doc  7700\n",
      "total K  256\n",
      "iter  0  doc  7800\n",
      "total K  264\n",
      "iter  0  doc  7900\n",
      "total K  267\n",
      "iter  0  doc  8000\n",
      "total K  270\n",
      "iter  0  doc  8100\n",
      "total K  272\n",
      "iter  0  doc  8200\n",
      "total K  272\n",
      "iter  0  doc  8300\n",
      "total K  272\n",
      "iter  0  doc  8400\n",
      "total K  272\n",
      "iter  0  doc  8500\n",
      "total K  272\n",
      "iter  0  doc  8600\n",
      "total K  273\n",
      "iter  0  doc  8700\n",
      "total K  275\n",
      "iter  0  doc  8800\n",
      "total K  275\n",
      "iter  0  doc  8900\n",
      "total K  276\n",
      "iter  0  doc  9000\n",
      "total K  276\n",
      "iter  0  doc  9100\n",
      "total K  277\n",
      "iter  0  doc  9200\n",
      "total K  278\n",
      "iter  0  doc  9300\n",
      "total K  280\n",
      "iter  0  doc  9400\n",
      "total K  280\n",
      "iter  0  doc  9500\n",
      "total K  283\n",
      "iter  0  doc  9600\n",
      "total K  289\n",
      "iter  0  doc  9700\n",
      "total K  292\n",
      "iter  0  doc  9800\n",
      "total K  293\n",
      "iter  0  doc  9900\n",
      "total K  293\n",
      "iter  0  doc  10000\n",
      "total K  293\n",
      "iter  0  doc  10100\n",
      "total K  293\n",
      "iter  0  doc  10200\n",
      "total K  294\n",
      "iter  0  doc  10300\n",
      "total K  294\n",
      "iter  0  doc  10400\n",
      "total K  294\n",
      "iter  0  doc  10500\n",
      "total K  294\n",
      "iter  0  doc  10600\n",
      "total K  294\n",
      "iter  0  doc  10700\n",
      "total K  294\n",
      "iter  0  doc  10800\n",
      "total K  295\n",
      "iter  0  doc  10900\n",
      "total K  295\n",
      "iter  0  doc  11000\n",
      "total K  295\n",
      "iter  0  doc  11100\n",
      "total K  296\n",
      "iter  0  doc  11200\n",
      "total K  296\n",
      "iter  0  doc  11300\n",
      "total K  296\n",
      "iter  0  doc  11400\n",
      "total K  301\n",
      "iter  0  doc  11500\n",
      "total K  302\n",
      "iter  0  doc  11600\n",
      "total K  304\n",
      "iter  0  doc  11700\n",
      "total K  308\n",
      "iter  0  doc  11800\n",
      "total K  311\n",
      "iter  0  doc  11900\n",
      "total K  312\n",
      "iter  0  doc  12000\n",
      "total K  313\n",
      "iter  0  doc  12100\n",
      "total K  314\n",
      "iter  0  doc  12200\n",
      "total K  314\n",
      "iter  0  doc  12300\n",
      "total K  315\n",
      "iter  0  doc  12400\n",
      "total K  315\n",
      "iter  0  doc  12500\n",
      "total K  315\n",
      "iter  0  doc  12600\n",
      "total K  315\n",
      "iter  0  doc  12700\n",
      "total K  315\n",
      "iter  0  doc  12800\n",
      "total K  315\n",
      "iter  0  doc  12900\n",
      "total K  315\n",
      "iter  0  doc  13000\n",
      "total K  315\n",
      "iter  0  doc  13100\n",
      "total K  315\n",
      "iter  0  doc  13200\n",
      "total K  316\n",
      "iter  0  doc  13300\n",
      "total K  320\n",
      "iter  0  doc  13400\n",
      "total K  323\n",
      "iter  0  doc  13500\n",
      "total K  327\n",
      "iter  0  doc  13600\n",
      "total K  329\n",
      "iter  0  doc  13700\n",
      "total K  330\n",
      "iter  0  doc  13800\n",
      "total K  331\n",
      "iter  0  doc  13900\n",
      "total K  332\n",
      "iter  0  doc  14000\n",
      "total K  332\n",
      "iter  0  doc  14100\n",
      "total K  333\n",
      "iter  0  doc  14200\n",
      "total K  333\n",
      "iter  0  doc  14300\n",
      "total K  333\n",
      "iter  0  doc  14400\n",
      "total K  333\n",
      "iter  0  doc  14500\n",
      "total K  334\n",
      "iter  0  doc  14600\n",
      "total K  334\n",
      "iter  0  doc  14700\n",
      "total K  334\n",
      "iter  0  doc  14800\n",
      "total K  334\n",
      "iter  0  doc  14900\n",
      "total K  334\n",
      "iter  0  doc  15000\n",
      "total K  334\n",
      "iter  0  doc  15100\n",
      "total K  335\n",
      "iter  0  doc  15200\n",
      "total K  337\n",
      "iter  0  doc  15300\n",
      "total K  342\n",
      "iter  0  doc  15400\n",
      "total K  346\n",
      "iter  0  doc  15500\n",
      "total K  347\n",
      "iter  0  doc  15600\n",
      "total K  348\n",
      "iter  0  doc  15700\n",
      "total K  348\n",
      "iter  0  doc  15800\n",
      "total K  349\n",
      "iter  0  doc  15900\n",
      "total K  350\n",
      "iter  0  doc  16000\n",
      "total K  350\n",
      "iter  0  doc  16100\n",
      "total K  351\n",
      "iter  0  doc  16200\n",
      "total K  351\n",
      "iter  0  doc  16300\n",
      "total K  353\n",
      "iter  0  doc  16400\n",
      "total K  353\n",
      "iter  0  doc  16500\n",
      "total K  353\n",
      "iter  0  doc  16600\n",
      "total K  353\n",
      "iter  0  doc  16700\n",
      "total K  353\n",
      "iter  0  doc  16800\n",
      "total K  353\n",
      "iter  0  doc  16900\n",
      "total K  353\n",
      "iter  0  doc  17000\n",
      "total K  353\n",
      "iter  0  doc  17100\n",
      "total K  353\n",
      "iter  0  doc  17200\n",
      "total K  359\n",
      "iter  0  doc  17300\n",
      "total K  360\n",
      "iter  0  doc  17400\n",
      "total K  361\n",
      "iter  0  doc  17500\n",
      "total K  361\n",
      "iter  0  doc  17600\n",
      "total K  362\n",
      "iter  0  doc  17700\n",
      "total K  363\n",
      "iter  0  doc  17800\n",
      "total K  363\n",
      "iter  0  doc  17900\n",
      "total K  363\n",
      "iter  0  doc  18000\n",
      "total K  363\n",
      "iter  0  doc  18100\n",
      "total K  364\n",
      "iter  0  doc  18200\n",
      "total K  364\n",
      "iter  0  doc  18300\n",
      "total K  364\n",
      "iter  0  doc  18400\n",
      "total K  364\n",
      "iter  0  doc  18500\n",
      "total K  364\n",
      "iter  0  doc  18600\n",
      "total K  364\n",
      "iter  0  doc  18700\n",
      "total K  364\n",
      "iter  0  doc  18800\n",
      "total K  364\n",
      "iter  0  doc  18900\n",
      "total K  365\n",
      "iter  0  doc  19000\n",
      "total K  368\n",
      "iter  0  doc  19100\n",
      "total K  372\n",
      "iter  0  doc  19200\n",
      "total K  373\n",
      "iter  0  doc  19300\n",
      "total K  373\n",
      "iter  0  doc  19400\n",
      "total K  374\n",
      "iter  0  doc  19500\n",
      "total K  374\n",
      "iter  0  doc  19600\n",
      "total K  374\n",
      "iter  0  doc  19700\n",
      "total K  374\n",
      "iter  0  doc  19800\n",
      "total K  375\n",
      "iter  0  doc  19900\n",
      "total K  375\n",
      "iter  0  doc  20000\n",
      "total K  375\n",
      "iter  0  doc  20100\n",
      "total K  375\n",
      "iter  0  doc  20200\n",
      "total K  375\n",
      "iter  0  doc  20300\n",
      "total K  376\n",
      "iter  0  doc  20400\n",
      "total K  377\n",
      "iter  0  doc  20500\n",
      "total K  377\n",
      "iter  0  doc  20600\n",
      "total K  377\n",
      "iter  0  doc  20700\n",
      "total K  377\n",
      "iter  0  doc  20800\n",
      "total K  378\n",
      "iter  0  doc  20900\n",
      "total K  383\n",
      "iter  0  doc  21000\n",
      "total K  383\n",
      "iter  0  doc  21100\n",
      "total K  383\n",
      "iter  0  doc  21200\n",
      "total K  384\n",
      "iter  0  doc  21300\n",
      "total K  384\n",
      "iter  0  doc  21400\n",
      "total K  384\n",
      "iter  0  doc  21500\n",
      "total K  385\n",
      "iter  0  doc  21600\n",
      "total K  385\n",
      "iter  0  doc  21700\n",
      "total K  385\n",
      "iter  0  doc  21800\n",
      "total K  385\n",
      "iter  0  doc  21900\n",
      "total K  385\n",
      "iter  0  doc  22000\n",
      "total K  385\n",
      "iter  0  doc  22100\n",
      "total K  385\n",
      "iter  0  doc  22200\n",
      "total K  385\n",
      "iter  0  doc  22300\n",
      "total K  386\n",
      "iter  0  doc  22400\n",
      "total K  386\n",
      "iter  0  doc  22500\n",
      "total K  386\n",
      "iter  0  doc  22600\n",
      "total K  386\n",
      "iter  0  doc  22700\n",
      "total K  387\n",
      "iter  0  doc  22800\n",
      "total K  389\n",
      "iter  0  doc  22900\n",
      "total K  389\n",
      "iter  0  doc  23000\n",
      "total K  390\n",
      "iter  0  doc  23100\n",
      "total K  391\n",
      "iter  0  doc  23200\n",
      "total K  391\n",
      "iter  0  doc  23300\n",
      "total K  391\n",
      "iter  0  doc  23400\n",
      "total K  391\n",
      "iter  0  doc  23500\n",
      "total K  392\n",
      "iter  0  doc  23600\n",
      "total K  392\n",
      "iter  0  doc  23700\n",
      "total K  392\n",
      "iter  0  doc  23800\n",
      "total K  392\n",
      "iter  0  doc  23900\n",
      "total K  392\n",
      "iter  0  doc  24000\n",
      "total K  392\n",
      "iter  0  doc  24100\n",
      "total K  392\n",
      "iter  0  doc  24200\n",
      "total K  393\n",
      "iter  0  doc  24300\n",
      "total K  393\n",
      "iter  0  doc  24400\n",
      "total K  393\n",
      "iter  0  doc  24500\n",
      "total K  393\n",
      "iter  0  doc  24600\n",
      "total K  393\n",
      "iter  0  doc  24700\n",
      "total K  395\n",
      "iter  0  doc  24800\n",
      "total K  396\n",
      "iter  0  doc  24900\n",
      "total K  398\n",
      "iter  0  doc  25000\n",
      "total K  399\n",
      "iter  0  doc  25100\n",
      "total K  399\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter  0  doc  25200\n",
      "total K  400\n",
      "iter  0  doc  25300\n",
      "total K  400\n",
      "iter  0  doc  25400\n",
      "total K  400\n",
      "iter  0  doc  25500\n",
      "total K  400\n",
      "iter  0  doc  25600\n",
      "total K  400\n",
      "iter  0  doc  25700\n",
      "total K  400\n",
      "iter  0  doc  25800\n",
      "total K  403\n",
      "iter  0  doc  25900\n",
      "total K  403\n",
      "iter  0  doc  26000\n",
      "total K  404\n",
      "iter  0  doc  26100\n",
      "total K  404\n",
      "iter  0  doc  26200\n",
      "total K  404\n",
      "iter  0  doc  26300\n",
      "total K  404\n",
      "iter  0  doc  26400\n",
      "total K  405\n",
      "iter  0  doc  26500\n",
      "total K  405\n",
      "iter  0  doc  26600\n",
      "total K  410\n",
      "iter  0  doc  26700\n",
      "total K  417\n",
      "iter  0  doc  26800\n",
      "total K  417\n",
      "iter  0  doc  26900\n",
      "total K  418\n",
      "iter  0  doc  27000\n",
      "total K  420\n",
      "iter  0  doc  27100\n",
      "total K  423\n",
      "iter  0  doc  27200\n",
      "total K  426\n",
      "iter  0  doc  27300\n",
      "total K  427\n",
      "iter  0  doc  27400\n",
      "total K  427\n",
      "iter  0  doc  27500\n",
      "total K  428\n",
      "iter  0  doc  27600\n",
      "total K  430\n",
      "iter  0  doc  27700\n",
      "total K  430\n",
      "iter  0  doc  27800\n",
      "total K  431\n",
      "iter  0  doc  27900\n",
      "total K  431\n",
      "iter  0  doc  28000\n",
      "total K  431\n",
      "iter  0  doc  28100\n",
      "total K  432\n",
      "iter  0  doc  28200\n",
      "total K  432\n",
      "iter  0  doc  28300\n",
      "total K  432\n",
      "iter  0  doc  28400\n",
      "total K  433\n",
      "iter  0  doc  28500\n",
      "total K  433\n",
      "iter  0  doc  28600\n",
      "total K  434\n",
      "iter  0  doc  28700\n",
      "total K  435\n",
      "iter  0  doc  28800\n",
      "total K  436\n",
      "iter  0  doc  28900\n",
      "total K  436\n",
      "iter  0  doc  29000\n",
      "total K  437\n",
      "iter  0  doc  29100\n",
      "total K  438\n",
      "iter  0  doc  29200\n",
      "total K  440\n",
      "iter  0  doc  29300\n",
      "total K  440\n",
      "iter  0  doc  29400\n",
      "total K  440\n",
      "iter  0  doc  29500\n",
      "total K  440\n",
      "iter  0  doc  29600\n",
      "total K  440\n",
      "iter  0  doc  29700\n",
      "total K  442\n",
      "iter  0  doc  29800\n",
      "total K  442\n",
      "iter  0  doc  29900\n",
      "total K  442\n",
      "iter  0  doc  30000\n",
      "total K  442\n",
      "iter  0  doc  30100\n",
      "total K  443\n",
      "iter  0  doc  30200\n",
      "total K  443\n",
      "iter  0  doc  30300\n",
      "total K  443\n",
      "iter  0   0.8084372998639955\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_14648/4005355408.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    331\u001b[0m                                 \u001b[0mcount_i\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    332\u001b[0m                                 \u001b[0mcount_ij\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 333\u001b[1;33m                                 \u001b[1;32mfor\u001b[0m \u001b[0mdocID\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    334\u001b[0m                                     \u001b[1;32mif\u001b[0m \u001b[0mtop_words\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdocID\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    335\u001b[0m                                         \u001b[0mcount_i\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "nmi_compara_batch = []\n",
    "mean_each_batch_nmi = {}\n",
    "iteration_nmi = []\n",
    "iteration_coherence = {}\n",
    "iteration_topics = {}\n",
    "iteration_NMI = {}\n",
    "for i_batch in compara_batch:\n",
    "    start = 0\n",
    "    end = 0\n",
    "    total_batch = None\n",
    "    if len(corpus) % i_batch == 0:\n",
    "        total_batch = int(len(corpus) / i_batch)\n",
    "    else:\n",
    "        total_batch = int(len(corpus) / i_batch) +1\n",
    "    mean_each_batch_nmi[i_batch] = []\n",
    "    for batch in range(total_batch):\n",
    "        ave_nmi = []\n",
    "        for mean_i in range(1):\n",
    "        \n",
    "            docID_assign_z = {}\n",
    "            m_z = {}\n",
    "            n_z = {}\n",
    "            n_w = {}\n",
    "            Topics = []\n",
    "            V = set()\n",
    "            D = set()\n",
    "            beta_topic_v = {}\n",
    "            beta_topic_sum = {}\n",
    "            topic_keyword = {}\n",
    "            \n",
    "            global_v_bar_k = {}\n",
    "            global_C_k= {}\n",
    "            global_mu_k= {}\n",
    "            global_psi_k= {}\n",
    "            global_nu_k= {}\n",
    "            global_kappa_k= {}\n",
    "            global_cov_k= {}\n",
    "            global_N= {}\n",
    "            global_inv_cov_k= {}\n",
    "            global_cov_det_k= {}\n",
    "            kappa0 = 0.01\n",
    "            dim = 50\n",
    "            vec_x = 1.0\n",
    "            nu0 = dim\n",
    "            psi = np.eye(dim)\n",
    "            mu0 = np.array([vec_x for i in range(dim)])[:,None]\n",
    "            \n",
    "            alpha = 0.03\n",
    "            #alpha = 0.003\n",
    "            \n",
    "            gamma = 30\n",
    "            gammaS = 0.03\n",
    "            #gamma = 0.03\n",
    "            #gammaS = 0.0000001\n",
    "            global_word = {}\n",
    "            global_important_word = None\n",
    "            \n",
    "            end = i_batch * (batch + 1)\n",
    "            if end > len(corpus):\n",
    "                end = len(corpus)\n",
    "            total_iter = 1\n",
    "            for iter in range(total_iter):\n",
    "                iteration_coherence[iter] = {}\n",
    "                iteration_topics[iter] = {}\n",
    "                iteration_NMI[iter] = {}\n",
    "                print(\"iter \",iter, \" total K \",len(Topics))\n",
    "#                 for k in Topics:\n",
    "#                     k_words = set()\n",
    "#                     for word in beta_topic_v[k]:\n",
    "#                         if beta_topic_v[k][word] == 1:\n",
    "#                             k_words.add(word)\n",
    "                    #print(\"k \",k , \"beta_topic_sum \",beta_topic_sum[k])\n",
    "                    #print(\"k \",k , \"n_w \",n_w[k])\n",
    "                #print()\n",
    "                for docID in range(start,end):\n",
    "                    if docID%100 == 0:\n",
    "                        print(\"iter \",iter, \" doc \",docID)\n",
    "                        print(\"total K \",len(Topics))\n",
    "#                     if docID%5000 == 0:\n",
    "#                         #print(\"iter \",iter, \" m_z \",m_z)\n",
    "#                         print(\"iter \",iter, \" total K \",Topics)\n",
    "#                         print()\n",
    "                    words = corpus[docID]\n",
    "                    D.discard(docID)\n",
    "                    if docID in docID_assign_z.keys():\n",
    "                        before_k = docID_assign_z[docID]\n",
    "                        m_z[before_k].discard(docID)\n",
    "                        for word in words:\n",
    "                            global_word[word] -= 1\n",
    "                            n_z[before_k][word] -= 1\n",
    "                            n_w[before_k] -=1\n",
    "#                         for word in n_z[before_k].keys():\n",
    "#                             if n_z[before_k][word] > 0:\n",
    "#                                 sampleBetaAssignment(before_k,word)\n",
    "                        k = before_k\n",
    "                        max_word_prob = n_w[k]\n",
    "                        if max_word_prob != 0:\n",
    "                            max_word_prob = max(n_z[k].values()) / max_word_prob\n",
    "\n",
    "                        for word in n_z[k].keys():\n",
    "                                if word not in beta_topic_v[k].keys():\n",
    "                                        beta_topic_v[k][word] = 0\n",
    "                                if n_z[k][word] > 0: \n",
    "                                    sampleBetaAssignment(k,word,iter,total_iter,max_word_prob)\n",
    "                                else:\n",
    "                                    if beta_topic_v[k][word] == 1:\n",
    "                                        beta_topic_sum[k] -= 1\n",
    "                                        beta_topic_v[k][word] = 0  \n",
    "                                        topic_keyword[k].discard(word)\n",
    "                        if docID % 500 == 0:\n",
    "                            global_important_word = sum_topic_word()\n",
    "                    else:\n",
    "                        before_k = -1\n",
    "                        \n",
    "                    if len(D) == 0 and len(V) == 0:\n",
    "                        choose_k = 0\n",
    "                        D.add(docID)\n",
    "                        docID_assign_z[docID] = choose_k\n",
    "                        if choose_k not in beta_topic_v.keys(): \n",
    "                            beta_topic_v[choose_k] = {}\n",
    "                        if choose_k not in beta_topic_sum.keys():\n",
    "                            beta_topic_sum[choose_k] = 0\n",
    "                        if choose_k not in m_z.keys():\n",
    "                            m_z[choose_k] = set()\n",
    "                        if choose_k not in topic_keyword.keys():\n",
    "                            topic_keyword[choose_k] = set()\n",
    "                        m_z[choose_k].add(docID)\n",
    "                        for word in words:\n",
    "                            if choose_k not in n_w.keys():\n",
    "                                n_w[choose_k] = 0\n",
    "                            if choose_k not in n_z.keys():\n",
    "                                n_z[choose_k] = {}\n",
    "                            if word not in n_z[choose_k].keys():\n",
    "                                n_z[choose_k][word] = 0\n",
    "                            if word not in beta_topic_v[choose_k].keys():\n",
    "                                beta_topic_v[choose_k][word] = 0\n",
    "                            if word not in global_word.keys():\n",
    "                                global_word[word] = 0\n",
    "                            global_word[word] += 1\n",
    "                            n_z[choose_k][word] += 1\n",
    "                            n_w[choose_k] += 1\n",
    "                            V.add(word)\n",
    "                        if choose_k not in Topics:\n",
    "                            Topics.append(choose_k)\n",
    "#                         if choose_k == Topics:\n",
    "#                             Topics += 1  \n",
    "                    else:\n",
    "                        log_pro = []\n",
    "                        \n",
    "                        must_update_flag = 0\n",
    "                        update_pro = 1\n",
    "                        not_update_pro = 1\n",
    "                        if_update_k = []\n",
    "                        for word in words:\n",
    "                            if word in global_important_word:\n",
    "                                must_update_flag = 1\n",
    "                                break\n",
    "                            x = vectors[word][:,None]\n",
    "                            LLcomp = (x - global_mu_k[0]).T.dot(global_inv_cov_k[0]).dot((x - global_mu_k[0])) \n",
    "                            pro = stats.chisqprob(LLcomp, dim)[0][0]\n",
    "                            update_pro *= pro\n",
    "                            not_update_pro *= (1 - pro)\n",
    "                            \n",
    "                        if must_update_flag == 0:\n",
    "                            if_update_k.append(not_update_pro)\n",
    "                            if_update_k.append(update_pro)\n",
    "                            sum_pro=sum(if_update_k)\n",
    "                            normalized_posterior = [i/sum_pro for i in if_update_k]    \n",
    "                            update_k = np.random.choice( 2 , 1, p=normalized_posterior)[0]\n",
    "                        else:\n",
    "                            update_k = 1\n",
    "                        \n",
    "                        choose_k = None\n",
    "                        if update_k == 1:\n",
    "                            for k in Topics:\n",
    "                                pro_k = len(m_z[k])\n",
    "                                if pro_k == 0:\n",
    "                                    log_pro.append(0)\n",
    "                                else:\n",
    "                                    i = 0\n",
    "                                    for word in words:\n",
    "                                        if word not in n_z[k].keys():\n",
    "                                            n_z[k][word] = 0\n",
    "                                        bias_flag = 0\n",
    "                                        if word in beta_topic_v[k].keys():\n",
    "                                            bias_flag = beta_topic_v[k][word]\n",
    "                                        for j in range(doc_words[docID][word]):\n",
    "                                            #pro_k *= (n_z[k][word]+ beta + j) / (n_w[k] + len(V)*beta + i) \n",
    "                                            pro_k *= (n_z[k][word]+ bias_flag*gamma + gammaS +j)/(n_w[k]+beta_topic_sum[k]*gamma+len(V)*gammaS +i) \n",
    "                                            i += 1\n",
    "                                    log_pro.append(pro_k)\n",
    "                            sum_pro=sum(log_pro)\n",
    "                            normalized_posterior = [i/sum_pro for i in log_pro]  \n",
    "                            select_k = None\n",
    "                            if iter == (total_iter - 1):\n",
    "                                select_k = normalized_posterior.index(max(normalized_posterior))\n",
    "\n",
    "                            else:\n",
    "                                select_k = np.random.choice( len(Topics) , 1, p=normalized_posterior)[0]   \n",
    "                            choose_k = Topics[select_k]\n",
    "                        else:\n",
    "                            choose_k = np.max(Topics) + 1\n",
    "                       \n",
    "                            \n",
    "                        D.add(docID)\n",
    "                        docID_assign_z[docID] = choose_k\n",
    "                        if choose_k not in m_z.keys():\n",
    "                            m_z[choose_k] = set()\n",
    "                        m_z[choose_k].add(docID)\n",
    "                        if choose_k not in beta_topic_v.keys(): \n",
    "                            beta_topic_v[choose_k] = {}\n",
    "                        if choose_k not in beta_topic_sum.keys():\n",
    "                            beta_topic_sum[choose_k] = 0\n",
    "                        if choose_k not in topic_keyword.keys():\n",
    "                            topic_keyword[choose_k] = set()\n",
    "                        for word in words:\n",
    "                            if choose_k not in n_w.keys():\n",
    "                                n_w[choose_k] = 0\n",
    "                            if choose_k not in n_z.keys():\n",
    "                                n_z[choose_k] = {}\n",
    "                            if word not in n_z[choose_k].keys():\n",
    "                                n_z[choose_k][word] = 0\n",
    "                            if word not in beta_topic_v[choose_k].keys():\n",
    "                                beta_topic_v[choose_k][word] = 0\n",
    "                            if word not in global_word.keys():\n",
    "                                global_word[word] = 0\n",
    "                            global_word[word] += 1\n",
    "                            n_z[choose_k][word] += 1\n",
    "                            n_w[choose_k] += 1\n",
    "                            V.add(word)\n",
    "                        if choose_k not in Topics:\n",
    "                            Topics.append(choose_k)\n",
    "                            \n",
    "                    count_k = []\n",
    "                    for k in Topics:\n",
    "                        if k in m_z.keys() and len(m_z[k]) == 0:\n",
    "                            m_z.pop(k, None)\n",
    "                            n_z.pop(k, None)\n",
    "                            n_w.pop(k, None)\n",
    "                            beta_topic_v.pop(k, None)\n",
    "                            beta_topic_sum.pop(k, None)\n",
    "                            beta_topic_v.pop(k, None)\n",
    "                            topic_keyword.pop(k, None)\n",
    "                            count_k.append(k)\n",
    "                    for k in count_k:\n",
    "                        Topics.remove(k)   \n",
    "                        \n",
    "                    k = choose_k\n",
    "                    max_word_prob = n_w[k]\n",
    "                    if max_word_prob != 0:\n",
    "                        max_word_prob = max(n_z[k].values()) / max_word_prob\n",
    "\n",
    "                    for word in n_z[k].keys():\n",
    "                            if word not in beta_topic_v[k].keys():\n",
    "                                    beta_topic_v[k][word] = 0\n",
    "                            if n_z[k][word] > 0: \n",
    "                                sampleBetaAssignment(k,word,iter,total_iter,max_word_prob)\n",
    "                            else:\n",
    "                                if beta_topic_v[k][word] == 1:\n",
    "                                    beta_topic_sum[k] -= 1\n",
    "                                    beta_topic_v[k][word] = 0   \n",
    "                                    topic_keyword[k].discard(word)\n",
    "                    if docID % 100 == 0:\n",
    "                        global_important_word = sum_topic_word()                \n",
    "#                 for k in Topics:\n",
    "#                     max_word_prob = n_w[k]\n",
    "#                     if max_word_prob != 0:\n",
    "#                         max_word_prob = max(n_z[k].values()) / max_word_prob\n",
    "\n",
    "#                     for word in n_z[k].keys():\n",
    "#                             if word not in beta_topic_v[k].keys():\n",
    "#                                     beta_topic_v[k][word] = 0\n",
    "#                             if n_z[k][word] > 0: \n",
    "#                                 sampleBetaAssignment(k,word,iter,total_iter,max_word_prob)\n",
    "#                             else:\n",
    "#                                 if beta_topic_v[k][word] == 1:\n",
    "#                                     beta_topic_sum[k] -= 1\n",
    "#                                     beta_topic_v[k][word] = 0\n",
    "                #spiltDocInTopic()\n",
    "#                 for k in Topics:\n",
    "#                     flag = 0\n",
    "#                     print(\"k \", k)\n",
    "#                     for word in beta_topic_v[k].keys():\n",
    "#                         if beta_topic_v[k][word] == 1:\n",
    "#                             flag = 1\n",
    "#                             print(\"import word:\",word)\n",
    "#                     print()\n",
    "                \n",
    "                from sklearn.metrics.cluster import normalized_mutual_info_score\n",
    "                nmi_sample = []\n",
    "                nmi_result = []\n",
    "                for key, value in news_labels.items():\n",
    "                    if key < end and key >= start:\n",
    "                        nmi_sample.append(value)\n",
    "                        nmi_result.append(docID_assign_z[key])\n",
    "                print(\"iter \",iter,\" \",normalized_mutual_info_score(np.array(nmi_sample), np.array(nmi_result)))\n",
    "                iteration_nmi.append(normalized_mutual_info_score(np.array(nmi_sample), np.array(nmi_result)))\n",
    "                \n",
    "                threshold_loop = [0, 3, 5, 10, 15, 20]\n",
    "                for threshold in threshold_loop:\n",
    "                    count = 0\n",
    "                    k_count = set()\n",
    "                    for k in m_z.keys():\n",
    "                        if len(m_z[k]) >= threshold:\n",
    "                            count += 1\n",
    "                            k_count.add(k)\n",
    "                    import operator\n",
    "                    post_topic_word = {}\n",
    "                    for k in k_count:\n",
    "                        post_topic_word[k] = {}\n",
    "                        for word in V:\n",
    "                            pro = None\n",
    "                            if word in beta_topic_v[k] and beta_topic_v[k][word] == 1:\n",
    "                                pro = (n_z[k][word]+ beta_topic_v[k][word]*gamma + gammaS) / (n_w[k] + beta_topic_sum[k]*gamma + len(V)*beta) \n",
    "                            elif word in n_z[k]:\n",
    "                                pro = (n_z[k][word]+ gammaS) / (n_w[k] + beta_topic_sum[k]*gamma + len(V)*beta) \n",
    "                            else:\n",
    "                                pro = (gammaS) / (n_w[k] + beta_topic_sum[k]*gamma + len(V)*beta) \n",
    "                            post_topic_word[k][word] = pro\n",
    "                        post_topic_word[k] = sorted(post_topic_word[k].items(), key=operator.itemgetter(1), reverse=True)\n",
    "                    top = 10\n",
    "                    topic_coherence = []\n",
    "                    for k in k_count:\n",
    "                        top_words = []\n",
    "                        pro = 0\n",
    "                        for i in range(top):\n",
    "                            top_words.append( post_topic_word[k][i][0] )\n",
    "\n",
    "                        for i in range(len(top_words)-1):\n",
    "                            for j in range(i+1,len(top_words) ):\n",
    "                                count_i = 0\n",
    "                                count_ij = 0\n",
    "                                for docID in range( len(corpus) ):\n",
    "                                    if top_words[i] in corpus[docID]:\n",
    "                                        count_i += 1\n",
    "                                        if top_words[j] in corpus[docID]:\n",
    "                                            count_ij += 1\n",
    "                                pro += log( (count_ij + 1) / count_i )\n",
    "                        topic_coherence.append(pro)\n",
    "                    iteration_coherence[iter][threshold] = np.mean(topic_coherence)\n",
    "                    iteration_topics[iter][threshold] = count\n",
    "                    \n",
    "                    nmi_sample = []\n",
    "                    nmi_result = []\n",
    "                    for k in k_count:\n",
    "                        for docID in m_z[k]:\n",
    "                            nmi_sample.append(news_labels[docID])\n",
    "                            nmi_result.append(docID_assign_z[docID])\n",
    "                    iteration_NMI[iter][threshold] = normalized_mutual_info_score(np.array(nmi_sample), np.array(nmi_result))\n",
    "                    \n",
    "                print(\"iter coherence: \",iteration_coherence)\n",
    "                print(\"iter topics: \",iteration_topics)\n",
    "                print(\"iter NMI: \",iteration_NMI)\n",
    "                \n",
    "                \n",
    "            from sklearn.metrics.cluster import normalized_mutual_info_score\n",
    "            nmi_sample = []\n",
    "            nmi_result = []\n",
    "            for key, value in news_labels.items():\n",
    "                if key < end and key >= start:\n",
    "                    nmi_sample.append(value)\n",
    "                    nmi_result.append(docID_assign_z[key])\n",
    "            ave_nmi.append(normalized_mutual_info_score(np.array(nmi_sample), np.array(nmi_result)))\n",
    "        mean_each_batch_nmi[i_batch].append(np.mean(ave_nmi))\n",
    "        start = end \n",
    "        if batch%1000 == 0:\n",
    "            print(\"i_batch\",i_batch , \"batch \",batch)\n",
    "            print(\" ave_nmi \",np.mean(ave_nmi))\n",
    "            print(\" mean_each_batch_nmi \",np.mean(mean_each_batch_nmi[i_batch]) ,\"max \", np.max(ave_nmi) ,\"min \", np.min(ave_nmi) )\n",
    "            print(\"total K \",len(Topics))\n",
    "            print(\"global_important_word \",len(global_important_word))\n",
    "            print(\"iteration_nmi \",iteration_nmi)\n",
    "            print(\"iteration_coherence \",iteration_coherence)\n",
    "            print()\n",
    "    nmi_compara_batch.append(np.mean(mean_each_batch_nmi[i_batch]))\n",
    "    print(compara_batch)\n",
    "    print(nmi_compara_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "outputPath = \"tweets-t.txt\"\n",
    "\n",
    "writer = open(outputPath, 'a')\n",
    "for i, docid in enumerate(docids):\n",
    "    documentID = docid\n",
    "    cluster = docID_assign_z[i]\n",
    "    writer.write(str(documentID) + \" \" + str(cluster) + \"\\n\")\n",
    "writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
